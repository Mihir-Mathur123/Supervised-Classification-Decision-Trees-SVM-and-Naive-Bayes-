{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes"
      ],
      "metadata": {
        "id": "0LHFb0TPcv0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.  What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "uyDMegrPdLYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Information Gain (IG) is a metric used to measure the effectiveness of an attribute in classifying data.  \n",
        "It is based on the concept of **entropy**, which measures the impurity or disorder of a dataset.\n",
        "\n",
        ">The formula for Information Gain is:  \n",
        ">```Information Gain = Entropy(Parent) - [Weighted Average] * Entropy(Children)```\n",
        ">- A higher Information Gain means the attribute provides more information about the target class.\n",
        ">- Decision Trees use IG to decide which feature to split on at each node — choosing the one with the **highest IG**.\n"
      ],
      "metadata": {
        "id": "4KLbg-Bic-G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "1exrElk1dj41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric | Formula | Range | Interpretation |\n",
        "|---------|----------|--------|----------------|\n",
        "| **Entropy** | \\(-p_1 \\log_2 p_1 - p_2 \\log_2 p_2 - ... - p_n \\log_2 p_n\\) | 0–1 | Measures impurity based on information theory |\n",
        "| **Gini Impurity** | \\(1 - \\sum p_i^2\\) | 0–0.5 | Measures how often a randomly chosen element is incorrectly labeled |\n",
        "\n",
        "> **Key Differences:**\n",
        ">- Entropy involves logarithmic computation (slower but information-theoretic).\n",
        ">- Gini is simpler and faster to compute.\n",
        ">- Both yield similar trees, but Gini often performs better in practice for large datasets."
      ],
      "metadata": {
        "id": "O0XxLUYxdw_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.  What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "uYH7JaiwePoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Pre-Pruning (also called **Early Stopping**) prevents a Decision Tree from becoming too complex.  \n",
        "Instead of fully growing the tree and pruning later, it stops the growth early when further splits don't significantly improve performance.\n",
        "\n",
        "> **Common Pre-Pruning Criteria:**\n",
        ">- Maximum depth (`max_depth`)\n",
        ">- Minimum samples per leaf (`min_samples_leaf`)\n",
        ">- Minimum information gain threshold\n",
        "\n",
        "> This helps reduce **overfitting** and improves **generalization**.\n"
      ],
      "metadata": {
        "id": "wxKQnn_YeXLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "pGDYP3thess5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
        "print(\"Feature Importances:\")\n",
        "print(importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjmiZHyee6Rp",
        "outputId": "cb6744c9-5313-4610-f63c-3e0611dfa601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.  What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "-XOzRBYSgi_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.  \n",
        "It finds the **optimal hyperplane** that best separates data points of different classes with **maximum margin**.\n",
        "\n",
        ">Key concepts:\n",
        ">- **Support Vectors:** Data points closest to the decision boundary.\n",
        ">- **Margin:** Distance between the hyperplane and support vectors.\n",
        ">- **Goal:** Maximize the margin for better generalization.\n"
      ],
      "metadata": {
        "id": "EwCYEPTKgoKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.  What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "YQzDGbyMg1ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The **Kernel Trick** allows SVM to perform nonlinear classification efficiently.  \n",
        "It maps data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        ">Commonly used kernels include:\n",
        ">- **Linear:** Suitable for data that is already linearly separable.\n",
        ">- **Polynomial:** Useful for adding interaction features between the original features.\n",
        ">- **RBF (Radial Basis Function):** Effective for handling complex, nonlinear decision boundaries. The formula for an RBF kernel is typically given by: ( K(x, x') = \\exp(-\\gamma ||x - x'||^2) ).\n"
      ],
      "metadata": {
        "id": "aubs0GQ2g7Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "gVqp4h8IheKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1FydfAlhli5",
        "outputId": "ce80b646-0f9f-405e-a938-350370b7532b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0000\n",
            "RBF Kernel Accuracy: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.  What is the Naïve Bayes classifier, and why is it called 'Naïve'?"
      ],
      "metadata": {
        "id": "tEbFoBH6hphH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Naïve Bayes is a **probabilistic classifier** based on **Bayes' Theorem**, assuming independence between features.\n",
        "\n",
        "Formula:  ( P(C|X) = \\frac{P(X|C) * P(C)}{P(X)} \\)\n",
        "\n",
        "> It’s called “Naïve” because it **assumes all features are independent**, which is rarely true in practice, but still performs surprisingly well for many problems.\n"
      ],
      "metadata": {
        "id": "sT8e_WW_hwc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
      ],
      "metadata": {
        "id": "tDVWOkToh-9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer :-\n",
        "| Type | Data Type | Use Case | Distribution |\n",
        "|------|------------|----------|---------------|\n",
        "| **GaussianNB** | Continuous | Real-valued features (e.g., Iris, Breast Cancer) | Normal distribution |\n",
        "| **MultinomialNB** | Discrete | Text classification (word counts) | Multinomial distribution |\n",
        "| **BernoulliNB** | Binary | Binary/boolean features (e.g., presence/absence of word) | Bernoulli distribution |\n"
      ],
      "metadata": {
        "id": "hvt71uIwiES4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.  Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "8wjwuZ0qiRZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"GaussianNB Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtOEkDDAiW9a",
        "outputId": "7e035a54-70b9-4667-f637-3fe27c34c911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB Accuracy: 0.9737\n"
          ]
        }
      ]
    }
  ]
}